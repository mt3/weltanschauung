\documentclass[10pt]{article}
\usepackage{moreverb}
\usepackage{algorithmic}
\usepackage{setspace} 
\usepackage{url}
\doublespacing

% Set Margins
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\hoffset=-.2in
\setlength{\textheight}{9.5in}
\setlength{\topmargin}{-1in}

\title{Poem Generation from Large Corpora Through Reappropriation}
\author{Nathaniel K Smith}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a new approach to computer poetry, presenting an algorithm
that performs a directed cut-up poem generation technique on a large corpus. The
technical implementation of the algorithm and its associated modules will be
discussed in detail. To provide background and motivation, a survey of existing
computer poetry generation techniques is presented with an analysis of their
ability to produce interesting results. To support my analysis, I offer a
discussion of how the technical implementation of computer poetry generation
affects the production of interesting results. I argue that the new, presented
algorithm fills both a technical and creative gap within the existing computer
poetry landscape. Finally, opportunities for further research are discussed.
\end{abstract}

\section{Introduction}
Innovation in the field of computer generated poetry has been sluggish at best,
most likely because of the historically small intersection between poets and
programmers \cite{Hart96}. It seems that most attempts made at `good' computer
poetry (A metric which is arguably even less measurable than in `good' poetry)
often fall prey to one of two problems: one, leaving too much to the computer,
thus cutting out human spontanaeity and creativity or two, treating the
computer as a bland random number generator and using an over-abundance of
human-prepared content.

I posit that there is a way to synthesize these two extremes by exploiting
human content easily accessible via the Internet. Huge changes in both the
amount of content stored online and the continuing ease of access to such
content mean that there is a massive store of thoughts, emotions, humour,
tradgedy, and other elements of `good' poetry just waiting to be used as
inspiration for the computer-poet. To this end, I present an algorithm that
`cuts up' a given, large corpus (presumably harvested from the Internet) and
produces a poem according to some desired output parameters.

This paper will discuss in detail my implementation of the algorithm and its
associated modules. To provide background and motivation, a survey of existing
computer poetry generation techniques is presented with an analysis of their
ability to produce interesting results. To support my analysis, I offer a
discussion of how the technical implementation of computer poetry generation
affects the production of interesting results. I argue that the new, presented
algorithm fills both a technical and creative gap within the existing computer
poetry landscape. Finally, opportunities for further research are discussed.

\subsection{Background}
\subsubsection{Two Categories}
My research into existing work in the field of computer poetry revealed two
general categories of techniques; one, I called \emp{Generative} and the other,
\emp{Templatized}. The former category relies on some input text and an
algorithm to produce new works, while the latter relies on a pre-defined
grammatical (or semantic) template to produce work from scratch (in other
words, without any input text).  

To adapt the summation found in \cite{Hart96}, Templatized computer poetry has this flow:
\begin{center}
\begin{boxed}
PROGRAM, TEMPLATE $\rightarrow$ TEXT
\end{boxed}
\end{center}
while Generative computer poetry looks like this:
\begin{center}
\begin{boxed}
TEXT $\rightarrow$ PROGRAM $\rightarrow$ TEXT
\end{boxed}
\end{center}

\subsubsection{Existing Projects}
Early attempts at computer poetry were Templatized techniques. The earliest
documented approach was Auto-Beatnik, which churned out randomly rendered poems
using grammatical structures and a vocabulary.  It was not until 1984 that
another serious attempt at poetry generation was publicised with the release of
a work by the program RACTER called \emph{The Policeman's Beard is
Half-Constructed}. At 66 pages, \emph{Policeman's Beard} was a much larger
effort than the handful of short poems output by Auto-Beatnik. However, the
generation process was essentially the same idea: the program called upon
built-in grammatical templates and a large vocabulary of english words
\cite{Chamb84}. Despite the 22 years between these two projects, they both
were, at their core, automated versions of the popular game Mad Libs.

A more recent Templatized technique is detailed in \cite{Manurung00} and
\cite{Manurung03}. This technique stochastically evolves a poem from a simple
phrase or set of simple phrases into an interesting poem using a set of
evaluation functions.

This approach is an even further departure from RACTER than Prose. The
technique, packaged in software named McGonagall, relies on the manipulation of
grammatical information using a Lexicalized Tree Adjoining Grammar (LTAG). The
LTAG is a derivation tree; in other words, it itself does not describe the
phrase structure of a sentence. Instead, it describes operations performed on a
set of phrases. Each node of the LTAG is an elementary tree describing some
phrase structure and each edge labels the location at which a child node should
be inserted into its parent node. These LTAG structures serve as a kind of
template from which poems are derived. Potential solutions are evolved using
the LTAGs and evaluated for things like meter, rhyme, and semantics.

An early and basic Generative approach is the Travesty Engine discussed in
\cite{Hart96}. This technique accepted an input text and reorganized it using a
Markov chain (a random generative process in which future states are selected
based only on the present state of the generation). Given some $n$, the
Travesty Engine reassembled an input text $t$ such that the result, $s$,
contained all the same $n$ length substrings in $t$. At $n = 1$ the program
simply shuffled the letters of $t$; but at about $n = 9$, $t$ begins to
resemble a grammatically correct rearrangment of the phrases in $s$. Thus, for
varying levels of $n$, new poetry could be produced from any input and this
basic algorithm without the need for any kind of external lexicon.

A recent technique, and one similar to the technique I am proposing, uses
Vector Space Modeling to produce Haiku using content found on blogs throughout
the Internet \cite{Wong08}. This method produces modern (in other words, not
strictly 5-7-5) Haiku poems using randomly accessed blog content. 

Before any utilization of blog data is used, a keyword lexicon is first built
using common Haiku terms. Using this lexicon, HTML output from a blog search
engine is parsed and sentences containing lexicon words are extracted and
converted into fragments according to English grammatical rules. Finally, longer
fragments are filtered out of the repository leaving only fragments of a length
suitable for use in Haiku.

Three keywords are randomly picked from the lexicon and used to further narrow
the sentence repository. The remaining fragments are then run through an
analysis stage in which sentence pairs are picked based on a vector
representation of their semantic similarity. Finally, the `best' three lines
are picked for the final Haiku, each one semantically involving one of the
three random keywords originally picked.

Another approach similar to my technique is the case based reasoning technique
developed in \cite{Gervas01}. This approach accepts prose sentences to be
converted to poetry. Largely, the process is Generative, making grammatical
changes to the input sentence as well as word substitutions. The target output
is specified by some existing stanza of the desired form (ie, a haiku). An
important part of the process, however, is a vocabulary of words that may be
substituted into the original prose to achieve the desired poetic structure,
making this approach a combination of Templatized and Generative techniques. A
user must supply this vocabulary for each poem desired.

\subsection{Analysis}
Throughout the course of this research it became evident that a common problem in
computer poetry was simply the question: Why? Templatized techniques tended to
rely almost exclusively on the human programmers designing the poetry
generation software; it seemed like the computer was little more than a set of
dice. In \cite{Manurung03}, where the Templatized technique is taken to an
extreme, the semantic scope of the produced work is still strictly limited to
what humans provide in the LTAG collection. It seems like computer poetry like
this isn't contributing anything to the field that a human couldn't already do
by hand.

The CBR approach suffers from similar problems despite being a partially Generative technique.
A large part of the creativity of the system depends on the words chosen for
the vocabulary used, meaning that a human must premeditate the content of
produced poems. Similarly, a human user must select the poetic verses used to
compare potential outputs to when generating a poem.

The question is easier to answer for Generative techniques. The Travesty
Engine, for example, produces new arrangements of pieces that humans may never
think to produce. It can reveal new meaning in an existing piece through purely
aleatoric means. Similarly, in the VSM Haiku generator, the produced poetry can
describe the zeitgeist of the blogs it searches (something that even Google
struggles to do). Clearly this poetry was doing something that a human couldn't
do: revealing meaning hidden within existing human prose. However, I submit
that in the former example the rearrangement is too simple; it doesn't go far
enough and its results become predictable after a fashion. In the latter
example, the final Haiku bear too little resemblance to their source material
to really articulate any kind of underlying meaning to the reader.

\section{Directed Cut-up Technique}
Thus, I present a new Generative technique inspired by the work of the poet and
author William Burroughs. This technique exploits the diversity and large
number of sentences found in large corpora--for example, Wikipedia, the online
articles of a newspaper, or online public domain books--by scanning the
sentences within them, profiling them, and then using them to populate a poem
based on rules input by a human user. Thus, the technique \emph{cuts up}, like
Burroughs \cite{wikiCutup}, some given corpus in order to produce a poetic
result. I argue that this technique produces interesting work, and present
several features of the resulting pieces that support this.

\subsection{Explication of Algorithm}
The technique is driven by a short algorithm  supported by several modules. It
is presented in pseudo-code in figure 1. The algorithm, given some corpus $C$
containing several thousands of lines of text, stores each sentence of the
corpus in a databse with associated syllabic and phonetic information. Using
any number of simple guidelines supplied by a user, it proceeds to construct a
poem using the sentences stored in the database. Should the algorithm be unable
to locate a sentence to satisfy some line of the desired poem--say, a sentence with
10 syllables containing the word \emph{aleatoric} and rhyming with the word
\emph{trumpet}--it \emph{weakens} the rule for that line, relaxing the criteria
used in the database lookup to find a sentence that at least closely resembles
one needed to meet the user's guidelines. Without this technique, termination
is not guaranteed; however, every rule eventually weakens to the trivial rule,
which is satisfied by any sentence.

\pagebreak
\onehalfspacing
\begin{figure}[here]
\begin{algorithmic}
\STATE Given some corpus $C$ and database handle $DB$
\STATE $L\gets$ normalize($C$)
\STATE insert($DB$, profile($L$))
\STATE $i \gets$ input()
\STATE $R \gets$ rulesParse($i$) \COMMENT{a list of lists of Rule objects}
\STATE $P \gets \emptyset$ \COMMENT{the output poem (a list of lines)}
\FOR{$r \in R$}
    \STATE $S \gets$ select($DB$, ruleSetToQuery($r$))
    \IF {not $S$}
        \WHILE {$r\prime \gets$ weaken($r\prime$)}
            \STATE $S\prime \gets$ select($DB$, ruleSetToQuery($r\prime$))
            \IF { $S\prime$ }
                \STATE $S \gets S\prime$
                \STATE break
            \ENDIF
        \ENDWHILE
    \ENDIF
    \STATE $s \gets$ random($S$)
    \STATE push($P, s$)
\ENDFOR
\STATE output($P$)
\end{algorithmic}
\caption{My algorithm}
\label{fig:algorithm}
\end{figure}
\doublespacing

\subsection{Technical Implementation}
The tools involved in the implementation of this algorithm are Perl 5.10 and
SQLite3. Perl was chosen due to its rich Lingua::EN repository which contained
many modules for parsing, splicing, and cleaning up English text. Used in the
implementation of this algorithm were the CPAN modules Splitter, Phoneme, and
Syllable of the Lingua::EN namespace. SQLite3 was chosen for its speed, in-RAM
database feature and portability.

Despite initial concerns about the ability of an interpreted language to
process enough text in a short time, the implementation ended up with favorable
benchmarks. Producing 10 line poems with an AB rhyme scheme and 10 syllables
per line from a SQLite3 database containing .4 million sentences consistently
took between .2 and .4 seconds. Parsing a 1 million line corpus and inserting
it into a SQLite3 database took 2.5 hours.  The former benchmark indicates that
the poem generation code will scale for larger corpora, but the latter shows
that refactoring or rewriting will be necessary to support multimillion line
corpora.

All code developed for this project is licensed under version 2.0 of the
Artistic License and freely available at
http://github.com/nathanielksmith/weltanschauung.

\subsection{Corpus Acquisition}
A key component of this approach is the presence of a large corpus. Years ago,
before the Internet expanded to its current volumnuous state, it is unlikely
that any such corpus would be publicly and freely available. Now, one can
consider the publicly accessible areas of the Internet as one huge corpus with
smaller selections of Internet content as subsets of this corpus.

For initial testing and debugging, a digitized version of a dozen short stories
by Jorge Luis Borges was manually developed and employed.

For later testing and protyping, a selection of 3,700 books from Project
Gutenberg were scraped using an automated script. From these books, 1,000,000
lines of text (out of a total 7,000,000) were analyzed, producing 433,850
profiled sentences stored in a database for use.

A scrape of several thousand Wikipedia pages was attempted, but the content
proved to be too diverse (in terms of encoding and structure) to be parsed
without lots of additional overhead.

\subsection{Corpus Processing}
For each sentence found in the input corpus, the following details are stored in a database:
\onehalfspacing
\begin{enumerate}
\item line number
\item rhyme sound
\item syllable count
\item number of words
\item sentence text
\end{enumerate}
\doublespacing

The rhyme sound is determined through analyzing the phonetic representation of
the last word of the sentence. Lingua::EN::Phoneme, a Perl module based on the
CMU Pronouncing Dictionary, is used to break apart the word into its
constituent phonemes. Its rhyme sound is defined to be the final vowel sound of
the phoneme list followed by any number of consonant sounds. For instance,
since the phonemes of the word \emph{bear} is B, AO1, R, its rhyme sound is
represented as `AO1R' while \emph{bears} would be represented as `AO1RS'.

Syllables are also counted using the Lingua::EN::Phoneme module. The syllable
count of a word is defined to be the number of vowel sounds (in other words,
phonemes with a string representation of length 3) found in its phonetic
representation.

Unfortunately, relying on the CMU dictionary means that there are occasionally
words which are not profiled in the listing. For these words, no rhyme sound is
recorded. For syllable counting, the Perl module Lingua::EN::Syllable is fallen
back to, which employs an intelligent vowel count technique to produce syllable
counts with about 80\% accuracy.

The line number is used both as a primary key for the sentence's database
representation as well as for determining line proximity.

\subsection{Rules System}
Once a corpus database is ready, user inputted guidelines for the desired
output are parsed into a list of lists of Rule objects that are capable of
weakening themselves and producing SQL clauses to be used in a SELECT
statement.

\subsubsection{Input to Rules Translation}
In the current implementation, output guidelines are specified through command
line arguments to a main driver program. With the exception of the length rule,
each rule specific on the command line describes a scheme--in other words, a
layout for each line of the poem.

For example, given the string ABABCC, the program will first pick a random
sound for each unique letter in the scheme and later use them to select
sentences with that rhyme sound for each line's corresponding rhyme label
modulo the number of lines. Thus, a 12 line poem with the scheme ABABCC would
produce a poem with ABABCCABABCC rhymes.

Similarly, one expressed syllable schemes using a comma separated list of
syllable counts. To specify a Haiku, one would input a length of 3 and a
syllable scheme of ``5,7,5".

Once each input rule is parsed, a list of Rule objects for each line of the
poem is created. Each rule object is a subclass of a basic Rule
class--supplying only a constructor and get/set functions for its weakness
level--and is initialized with the parameters set on the command line.

As an example, the command line arguments:
\begin{verbatim}
--rhyme=``AB" --length=``5,10" --syllables=``8" --fuzzy\_keyword=``turntable,alphabet"
\end{verbatim}
would translate to the following data structure (with each rule object
represented by the resulting constructor call) after selecting `AO1' as the
rhyme sound for A and `EH1' as the rhyme sound for B:

\begin{verbatim}
(
    (Rule::Rhyme(`A01'), Rule::Syllable(5), Rule::Keyword(`turntable', `fuzzy')),
    (Rule::Rhyme(`EH1'), Rule::Syllable(10), Rule::Keyword(`alphabet', `fuzzy')),
    (Rule::Rhyme(`A01'), Rule::Syllable(5), Rule::Keyword(`turntable', `fuzzy')),
    (Rule::Rhyme(`EH1'), Rule::Syllable(10), Rule::Keyword(`alphabet', `fuzzy')),
    (Rule::Rhyme(`A01'), Rule::Syllable(5), Rule::Keyword(`turntable', `fuzzy'))
)
\end{verbatim}

\subsubsection{Rule Class}
Though not included in the algorithm's original design, it became clear that
the only way to properly organize the rule code was through an object oriented
approach. I designed a Rule class which keeps track of a `weakness' level--the
number of times the rule has been liberalized by the algorithm--and provides
functions for weakening the rule returning a SQL clause that describes the rule
in its current state.

Each rule object's get\_clause function returns SQL suitable for appending to a
SELECT statement's WHERE clause. For example, the unweakened SQL for a
Rule::Syllable object initialized with an argument of 5 would be
`(num\_syllables = 5)'. Thus, the ruleSetToQuery needs only iterate over the
rule objects for any line of the poem it's constructing and join each object's
get\_clause output with the string ` AND ' to result in a statement like:

\begin{verbatim}
SELECT sentence 
    FROM lines 
    WHERE (num_syllables = 5) 
    AND (end_rhyme_sound = 'AO1') 
    AND (sentence LIKE '% burrow %')
\end{verbatim}

Once weakened, a rule object's get\_clause function returns SQL modified to be
more general. Instead of giving weights to any kind of rule, each rule object
in the rule set for a given line of a poem is equally likely to be randomly
weakened. To illustrate the weakening process, here is a trace of the SQL
generated for each line of a sample 10-line AB scheme poem using a small (600
sentence) corpus:

\begin{verbatim}
SELECT sentence FROM lines WHERE...

(end_rhyme_sound=\'AE1N\')  AND  (num_syllables = 10)
(end_rhyme_sound=\'AE1N\')  AND  (num_syllables BETWEEN 10-1 AND 10+1)
(end_rhyme_sound=\'AE1N\')  AND  (num_syllables BETWEEN 10-2 AND 10+2)
(end_rhyme_sound=\'AE2N\')  AND  (num_syllables BETWEEN 10-2 AND 10+2)
(end_rhyme_sound=\'AE0N\')  AND  (num_syllables BETWEEN 10-2 AND 10+2)
(end_rhyme_sound=\'AE0N\')  AND  (num_syllables BETWEEN 10-3 AND 10+3)
(1) AND (num_syllables BETWEEN 10-3 AND 10+3)
(end_rhyme_sound=\'AH0L\') AND (num_syllables = 10)
(end_rhyme_sound=\'AH1L\') AND (num_syllables = 10)
(end_rhyme_sound=\'AH1L\') AND (num_syllables BETWEEN 10-1 AND 10+1)
(end_rhyme_sound=\'AH2L\') AND (num_syllables BETWEEN 10-1 AND 10+1)
(1) AND (num_syllables BETWEEN 10-1 AND 10+1)
(end_rhyme_sound=\'AE1N\') AND (num_syllables = 10)
(end_rhyme_sound=\'AE2N\') AND (num_syllables = 10)
(end_rhyme_sound=\'AE2N\') AND (num_syllables BETWEEN 10-1 AND 10+1)
(end_rhyme_sound=\'AE0N\') AND (num_syllables BETWEEN 10-1 AND 10+1)
(1) AND (num_syllables BETWEEN 10-1 AND 10+1)
(end_rhyme_sound=\'AH0L\') AND (num_syllables = 10)
(end_rhyme_sound=\'AH0L\') AND (num_syllables BETWEEN 10-1 AND 10+1)
(end_rhyme_sound=\'AE1N\') AND (num_syllables = 10)
(end_rhyme_sound=\'AE1N\') AND (num_syllables BETWEEN 10-1 AND 10+1)
(end_rhyme_sound=\'AE1N\') AND (num_syllables BETWEEN 10-2 AND 10+2)
(end_rhyme_sound=\'AE1N\') AND (num_syllables BETWEEN 10-3 AND 10+3)
(end_rhyme_sound=\'AE2N\') AND (num_syllables BETWEEN 10-3 AND 10+3)
(end_rhyme_sound=\'AE2N\') AND (1)
(end_rhyme_sound=\'AE0N\') AND (1)
(end_rhyme_sound=\'AE0N\') AND (1)
(end_rhyme_sound=\'AE0N\') AND (1)
(1) AND (1)
(end_rhyme_sound=\'AH0L\') AND (num_syllables = 10)
(end_rhyme_sound=\'AH1L\') AND (num_syllables = 10)
(end_rhyme_sound=\'AH2L\') AND (num_syllables = 10)
(end_rhyme_sound=\'AH2L\') AND (num_syllables BETWEEN 10-1 AND 10+1)
(1) AND (num_syllables BETWEEN 10-1 AND 10+1)
(end_rhyme_sound=\'AE1N\') AND (num_syllables = 10)
(end_rhyme_sound=\'AE1N\') AND (num_syllables BETWEEN 10-1 AND 10+1)
(end_rhyme_sound=\'AE1N\') AND (num_syllables BETWEEN 10-2 AND 10+2)
(end_rhyme_sound=\'AE1N\') AND (num_syllables BETWEEN 10-3 AND 10+3)
(end_rhyme_sound=\'AE2N\') AND (num_syllables BETWEEN 10-3 AND 10+3)
(end_rhyme_sound=\'AE0N\') AND (num_syllables BETWEEN 10-3 AND 10+3)
(1) AND (num_syllables BETWEEN 10-3 AND 10+3)

Your illustrious ancestor?
First: The Library exists ab aeterno
Let heaven exist, though my place be in hell
The consequences were incalculable
Philosophic controversy usurps a good part of the novel
Wordlessly, I returned the sheet
and I hung up the receiver
\end{verbatim}

The algorithm struggled to find good candidate sentences given the size of the
corpus--this was intentional to show how rules weaken. Note that the trivial
rule manifests itself as simple `(1)', a clause sufficient to select any
sentence.

\subsubsection{Implemented Rules}
At publication, the following rules are implemented:
\onehalfspacing
\begin{enumerate}
\item Rhyme: Match the rhyme sound of the last word of a sentence
\item Syllable: Match the syllable count of a sentence
\item Keyword: Match lines that either contain a given word (exact) or are near such a line (fuzzy)
\end{enumerate}
\doublespacing

\section{Results}
\subsection{Selected Outputs}
\subsubsection{Haiku}
\subsubsection{Sonnet}
\subsubsection{Limerick}
\subsubsection{Sestina}
\subsubsection{Miscellaneous}
\subsection{Defense of Technique}

\subsubsection{Semantic Consistency}
By using one or more topics as rules in the input to the software one can
topically flavor the output without restricting it. Sentences relevant to the
selected topic will be used to produce the final piece. The reader should be
able to get a sense of what each produced poem is "about." It is important to
note that this semantic consistency does not undermine juxtaposition, one of
the important aspects of computer poetry \cite{Hart96}. The output will still
contain the often jarring combination of seemingly otherwise unrelated phrases
despite their semantic consistency.

\subsubsection{Variable Structure}
Poems produced by my technique can scale from completely arbitrary, random
productions to very narrowly defined poetic structures (like sonnets, Haiku, or
limericks). The rules engine system allows for great flexibility and is a key
component of this technique. Its success, however, relies on the theory that
large enough corpora will contain sentences diverse enough to satisfy any input
rules.

\subsubsection{Human Mediated, Not Human Authored}
A key aspect of my approach is that, while all of the content used to produce
works is written by humans, the meaning of the content is subverted and altered
by the reappropriation process to create something new in which the humans and
computers involved both played integral roles in the poetic production. I refer
to the technique as being \emph{human mediated} as humans provide the
media--prose sentences--while the computer acts as the artist. In this case,
however, the computer is more like an artist of reappropriation like a DJ,
collage artist, or cut-up author.

\subsubsection{Conclusion}
Thus, I argue that this approach is a versatile one that employs the computer in a
necessary and novel way to produce interesting poetry. The rules system is
sufficiently `hands-off' such that a human merely suggests the final poetic
structure of a piece while the computer performs a task that is humanly
impossible. Thus, the `Why?' of this approach is: new, creative work is
discovered and enjoyed with the necessary help of a computer.

\section{Further Research}
\subsubsection{Missing Pieces}
The most glaring lacking in this implementation is the absence of a scansion
rule. Without this rule, most major poetic forms cannot be approximated.

Second most importantly, I need to develop a faster way of acquiring and
profiling large corpora. It seems that refactoring and parallelizing parts of
the existing code would be the most expedient way to accomplish this.

Finally, the resulting pieces need to be cleaned up in some automated
way--stray punctuation should be corrected, as should erroneous characters that
survived the profiling process (like newlines).
\subsubsection{Applications}
The interestingness of this approach's output depends largely on its corpus. Some areas which would prove fruitful would be:
\onehalfspacing
\begin{enumerate}
\item Specific selections of multiple poets
\end{enumerate}
\doublespacing

\bibliography{paper}
\bibliographystyle{apa}

\end{document}
