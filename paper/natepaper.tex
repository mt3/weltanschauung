\documentclass[10pt]{article}
\usepackage{moreverb}
\usepackage{algorithmic}
\usepackage{setspace} 
\usepackage{url}
\doublespacing

% Set Margins
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\hoffset=-.2in
\setlength{\textheight}{9.5in}
\setlength{\topmargin}{-1in}

\title{Poem Generation from Large Corpora through Reappropriation}
\author{Nathaniel K Smith}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a new approach to computer poetry, presenting an algorithm
that performs a directed cut-up poem generation technique on a large corpus. The
technical implementation of the algorithm and its associated modules will be
discussed in detail. To provide background and motivation, a survey of existing
computer poetry generation techniques is presented with an analysis of their
ability to produce interesting results. To support my analysis, I offer a
discussion of how the technical implementation of computer poetry generation
affects the production of interesting results. I argue that the new, presented
algorithm fills both a technical and creative gap within the existing computer
poetry landscape. Finally, opportunities for further research are discussed.
\end{abstract}

\section{Introduction}
Innovation in the field of computer generated poetry has been sluggish at best,
most likely because of the historically small intersection between poets and
programmers \cite{Hart96}. It seems that most attempts made at 'good' computer
poetry (A metric which is arguably even less measurable than in 'good' poetry)
often fall prey to one of two problems: one, leaving too much to the computer,
thus cutting out human spontanaeity and creativity or two, treating the
computer as a bland random number generator and using an over-abundance of
human-prepared content.

I posit that there is a way to synthesize these two extremes by exploiting
human content easily accessible via the Internet. Huge changes in both the
amount of content stored online and the continuing ease of access to such
content mean that there is a massive store of thoughts, emotions, humour,
tradgedy, and other elements of 'good' poetry just waiting to be used as
inspiration for the computer-poet. To this end, I present an algorithm that
'cuts up' a given, large corpus (presumably harvested from the Internet) and
produces a poem according to some desired output parameters.

This paper will discuss in detail my implementation of the algorithm and its
associated modules. To provide background and motivation, a survey of existing
computer poetry generation techniques is presented with an analysis of their
ability to produce interesting results. To support my analysis, I offer a
discussion of how the technical implementation of computer poetry generation
affects the production of interesting results. I argue that the new, presented
algorithm fills both a technical and creative gap within the existing computer
poetry landscape. Finally, opportunities for further research are discussed.

\section{Background and Motivation}
\subsection{Two Categories}
My research into existing work in the field of computer poetry revealed two
general categories of techniques; one, I called \emp{Generative} and the other,
\emp{Templatized}. The former category relies on some input text and an
algorithm to produce new works, while the latter relies on a pre-defined
grammatical (or semantic) template to produce work from scratch (in other
words, without any input text).  

To adapt the summation found in \cite{Hart96}, Templatized computer poetry has this flow:
\begin{center}
\begin{boxed}
PROGRAM, TEMPLATE $\rightarrow$ TEXT
\end{boxed}
\end{center}
while Generative computer poetry looks like this:
\begin{center}
\begin{boxed}
TEXT $\rightarrow$ PROGRAM $\rightarrow$ TEXT
\end{boxed}
\end{center}

\subsection{Existing Projects}
\subsubsection{Auto-Beatnik and RACTER}
Early attempts at computer poetry were Templatized techniques. The earliest
documented approach was Auto-Beatnik, which churned out randomly rendered poems
using grammatical structures and a vocabulary.  It was not until 1984 that
another serious attempt at poetry generation was publicised with the release of
a work by the program RACTER called \emph{The Policeman's Beard is
Half-Constructed}. At 66 pages, \emph{Policeman's Beard} was a much larger
effort than the handful of short poems output by Auto-Beatnik. However, the
generation process was essentially the same idea: the program called upon
built-in grammatical templates and a large vocabulary of english words
\cite{Chamb84}. Despite the 22 years between these two projects, they both
were, at their core, automated versions of the popular game Mad Libs.

\subsubsection{McGonagall}
A recent, and more advanced Templatized technique was the subject of the
dissertation of \cite{Manurung03}. Contained within a software package called
McGonagall, this technique stores "templates" in the form of Lexicalized Tree
Adjoining Grammars. In other words, instead of storing hardcoded grammatical
structures, a collection of LTAGs representing words and their semantics, part
of speech, and phonetic attributes are used to evolve candidate poetic works.
These works are evaluated by functions that check for certain poetic features
(like rhyming and alliteration) and final "solutions" are presented to the
user.

\subsubsection{The Travesty Engine}
An early and basic Generative approach is the Travesty Engine discussed in
\cite{Hart96}. This technique accepted an input text and reorganized it using a
Markov chain (a random generative process in which future states are selected
based only on the present state of the generation). Given some $n$, the
Travesty Engine reassembled an input text $t$ such that the result, $s$,
contained all the same $n$ length substrings in $t$. At $n = 1$ the program
simply shuffled the letters of $t$; but at about $n = 9$, $t$ begins to
resemble a grammatically correct rearrangment of the phrases in $s$. Thus, for
varying levels of $n$, new poetry could be produced from any input and this
basic algorithm without the need for any kind of external lexicon.

\subsubsection{Vector Space Modeling}
A recent technique, and one similar to the technique I am proposing, uses
Vector Space Modeling to produce Haiku using content found on blogs throughout
the Internet \cite{Wong08}. This method produces modern (in other words, not
strictly 5-7-5) Haiku poems using randomly accessed blog content. 

Before any utilization of blog data is used, a keyword lexicon is first built
using common Haiku terms. Using this lexicon, HTML output from a blog search
engine is parsed and sentences containing lexicon words are extracted and
converted into fragments according to English grammatical rules. Finally, longer
fragments are filtered out of the repository leaving only fragments of a length
suitable for use in Haiku.

Three keywords are randomly picked from the lexicon and used to further narrow
the sentence repository. The remaining fragments are then run through an
analysis stage in which sentence pairs are picked based on a vector
representation of their semantic similarity. Finally, the 'best' three lines
are picked for the final Haiku, each one semantically involving one of the
three random keywords originally picked.

\subsubsection{Case Based Reasoning Approach}
Another approach similar to my technique is the case based reasoning technique
developed in \cite{Gervas01}.

\subsection{Analysis}
\subsubsection{Effect of Technical Implementation on Composed Work}
\subsubsection{The Interestingness Problem}
\subsection{Motivation}
Throughout the course of my research it became evident that a common problem in
computer poetry was simply the question: Why? Templatized techniques tended to
rely almost exclusively on the human programmers designing the poetry
generation software; it seemed like the computer was little more than a set of
dice. In \cite{Manurung03}, where the Templatized technique is taken to an
extreme, the semantic scope of the produced work is still strictly limited to
what humans provide in the LTAG collection. It seems like computer poetry like
this isn't contributing anything to the field that a human couldn't already do
by hand.

The question is easier to answer for Generative techniques. The Travesty
Engine, for example, produces new arrangements of pieces that humans may never
think to produce. It can reveal new meaning in an existing piece through purely
aleatoric means. Similarly, in the VSM Haiku generator, the produced poetry can
describe the zeitgeist of the blogs it searches (something that even Google
struggles to do). Clearly this poetry was doing something that a human couldn't
do: revealing meaning hidden within existing human prose. However, I submit
that in the former example the rearrangement is too simple; it doesn't go far
enough and its results become predictable after a fashion. In the latter
example, the final Haiku bear too little resemblance to their source material
to really articulate any kind of underlying meaning to the reader.

Thus, I propose a new Generative technique inspired by the work of the poet and
author William Burroughs. This technique exploits the diversity and large
number of sentences found in large corpora--for example, Wikipedia, the online
articles of a newspaper, or online public domain books--by scanning the
sentences within them, profiling them, and then using them to populate a poem
based on rules input by a human user. Thus, the technique \emph{cuts up}, like
Burroughs \cite{wikiCutup}, some given corpus in order to produce a poetic
result. I argue that this technique produces interesting work, and present
several features of the resulting pieces that support this.

\subsubsection{Semantic Consistency}
By using one or more topics as rules in the input to the software one can
topically flavor the output without restricting it. Sentences relevant to the
selected topic will be used to produce the final piece. The reader should be
able to get a sense of what each produced poem is "about." It is important to
note that this semantic consistency does not undermine juxtaposition, one of
the important aspects of computer poetry \cite{Hart96}. The output will still
contain the often jarring combination of seemingly otherwise unrelated phrases
despite their semantic consistency.

\subsubsection{Variable Structure}
Poems produced by my technique can scale from completely arbitrary, random
productions to very narrowly defined poetic structures (like sonnets, Haiku, or
limericks). The rules engine system allows for great flexibility and is a key
component of this technique. Its success, however, relies on the theory that
large enough corpora will contain sentences diverse enough to satisfy any input
rules.

\subsubsection{Human Mediated, Not Human Authored}
A key aspect of my approach is that, while all of the content used to produce
works is written by humans, the meaning of the content is subverted and altered
by the reappropriation process to create something new in which the humans and
computers involved both played integral roles in the poetic production. I refer
to the technique as being \emph{human mediated} as humans provide the
media--prose sentences--while the computer acts as the artist. In this case,
however, the computer is more like an artist of reappropriation like a DJ,
collage artist, or cut-up author.
\section{Directed Cut-up Technique}
\subsection{Explication of Algorithm}
\pagebreak
\onehalfspacing
\begin{figure}[here]
\begin{algorithmic}
\STATE Given some corpus $C$ and database handle $DB$
\STATE $L\gets$ normalize($C$)
\STATE insert($DB$, profile($L$))
\STATE $i \gets$ input()
\STATE $R \gets$ rulesParse($i$)
\STATE $P \gets \emptyset$
\FOR{$r \in R$}
    \STATE $S \gets$ select($DB$, ruleToQuery($r$))
    \IF {not $S$}
        \WHILE {$r\prime \gets$ weaken($r\prime$)}
            \STATE $S\prime \gets$ select($DB$, ruleToQuery($r\prime$))
            \IF { $S\prime$ }
                \STATE $S \gets S\prime$
                \STATE break
            \ENDIF
        \ENDWHILE
    \ENDIF
    \STATE $s \gets$ random($S$)
    \STATE push($P, s$)
\ENDFOR
\STATE output($P$)
\end{algorithmic}
\caption{My algorithm}
\label{fig:algorithm}
\end{figure}
\doublespacing
\subsection{Corpus Acquisition}
\subsection{Corpus Processing}
\subsection{Rules System}
\subsubsection{Input to Rules Translation}
\subsubsection{Rules to Query Translation}
\subsection{Final Composition}
\section{Results}
\subsection{Selected outputs}
\subsection{Defense of Technique}
\section{Further Research}

\bibliography{paper}
\bibliographystyle{apa}

\end{document}
