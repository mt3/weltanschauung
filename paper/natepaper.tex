\documentclass[10pt]{article}
\usepackage{moreverb}
\usepackage{algorithmic}
\usepackage{setspace} 
\usepackage{url}
\doublespacing

% Set Margins
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\hoffset=-.2in
\setlength{\textheight}{9.5in}
\setlength{\topmargin}{-1in}

\title{Poem Generation from Large Corpora through Reappropriation}
\author{Nathaniel K Smith}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a new approach to computer poetry, presenting an algorithm
that performs a directed cut-up poem generation technique on a large corpus. The
technical implementation of the algorithm and its associated modules will be
discussed in detail. To provide background and motivation, a survey of existing
computer poetry generation techniques is presented with an analysis of their
ability to produce interesting results. To support my analysis, I offer a
discussion of how the technical implementation of computer poetry generation
affects the production of interesting results. I argue that the new, presented
algorithm fills both a technical and creative gap within the existing computer
poetry landscape. Finally, opportunities for further research are discussed.
\end{abstract}

\section{Introduction}
Innovation in the field of computer generated poetry has been sluggish at best,
most likely because of the historically small intersection between poets and
programmers \cite{Hart96}. It seems that most attempts made at 'good' computer
poetry (A metric which is arguably even less measurable than in 'good' poetry)
often fall prey to one of two problems: one, leaving too much to the computer,
thus cutting out human spontanaeity and creativity or two, treating the
computer as a bland random number generator and using an over-abundance of
human-prepared content.

I posit that there is a way to synthesize these two extremes by exploiting
human content easily accessible via the Internet. Huge changes in both the
amount of content stored online and the continuing ease of access to such
content mean that there is a massive store of thoughts, emotions, humour,
tradgedy, and other elements of 'good' poetry just waiting to be used as
inspiration for the computer-poet. To this end, I present an algorithm that
'cuts up' a given, large corpus (presumably harvested from the Internet) and
produces a poem according to some desired output parameters.

This paper will discuss in detail my implementation of the algorithm and its
associated modules. To provide background and motivation, a survey of existing
computer poetry generation techniques is presented with an analysis of their
ability to produce interesting results. To support my analysis, I offer a
discussion of how the technical implementation of computer poetry generation
affects the production of interesting results. I argue that the new, presented
algorithm fills both a technical and creative gap within the existing computer
poetry landscape. Finally, opportunities for further research are discussed.

\subsection{Background}
\subsubsection{Two Categories}
My research into existing work in the field of computer poetry revealed two
general categories of techniques; one, I called \emp{Generative} and the other,
\emp{Templatized}. The former category relies on some input text and an
algorithm to produce new works, while the latter relies on a pre-defined
grammatical (or semantic) template to produce work from scratch (in other
words, without any input text).  

To adapt the summation found in \cite{Hart96}, Templatized computer poetry has this flow:
\begin{center}
\begin{boxed}
PROGRAM, TEMPLATE $\rightarrow$ TEXT
\end{boxed}
\end{center}
while Generative computer poetry looks like this:
\begin{center}
\begin{boxed}
TEXT $\rightarrow$ PROGRAM $\rightarrow$ TEXT
\end{boxed}
\end{center}

\subsubsection{Existing Projects}
Early attempts at computer poetry were Templatized techniques. The earliest
documented approach was Auto-Beatnik, which churned out randomly rendered poems
using grammatical structures and a vocabulary.  It was not until 1984 that
another serious attempt at poetry generation was publicised with the release of
a work by the program RACTER called \emph{The Policeman's Beard is
Half-Constructed}. At 66 pages, \emph{Policeman's Beard} was a much larger
effort than the handful of short poems output by Auto-Beatnik. However, the
generation process was essentially the same idea: the program called upon
built-in grammatical templates and a large vocabulary of english words
\cite{Chamb84}. Despite the 22 years between these two projects, they both
were, at their core, automated versions of the popular game Mad Libs.

A more recent Templatized technique is detailed in \cite{Manurung00} and
\cite{Manurung03}. This technique stochastically evolves a poem from a simple
phrase or set of simple phrases into an interesting poem using a set of
evaluation functions.

This approach is an even further departure from RACTER than Prose. The
technique, packaged in software named McGonagall, relies on the manipulation of
grammatical information using a Lexicalized Tree Adjoining Grammar (LTAG). The
LTAG is a derivation tree; in other words, it itself does not describe the
phrase structure of a sentence. Instead, it describes operations performed on a
set of phrases. Each node of the LTAG is an elementary tree describing some
phrase structure and each edge labels the location at which a child node should
be inserted into its parent node. These LTAG structures serve as a kind of
template from which poems are derived. Potential solutions are evolved using
the LTAGs and evaluated for things like meter, rhyme, and semantics.

An early and basic Generative approach is the Travesty Engine discussed in
\cite{Hart96}. This technique accepted an input text and reorganized it using a
Markov chain (a random generative process in which future states are selected
based only on the present state of the generation). Given some $n$, the
Travesty Engine reassembled an input text $t$ such that the result, $s$,
contained all the same $n$ length substrings in $t$. At $n = 1$ the program
simply shuffled the letters of $t$; but at about $n = 9$, $t$ begins to
resemble a grammatically correct rearrangment of the phrases in $s$. Thus, for
varying levels of $n$, new poetry could be produced from any input and this
basic algorithm without the need for any kind of external lexicon.

A recent technique, and one similar to the technique I am proposing, uses
Vector Space Modeling to produce Haiku using content found on blogs throughout
the Internet \cite{Wong08}. This method produces modern (in other words, not
strictly 5-7-5) Haiku poems using randomly accessed blog content. 

Before any utilization of blog data is used, a keyword lexicon is first built
using common Haiku terms. Using this lexicon, HTML output from a blog search
engine is parsed and sentences containing lexicon words are extracted and
converted into fragments according to English grammatical rules. Finally, longer
fragments are filtered out of the repository leaving only fragments of a length
suitable for use in Haiku.

Three keywords are randomly picked from the lexicon and used to further narrow
the sentence repository. The remaining fragments are then run through an
analysis stage in which sentence pairs are picked based on a vector
representation of their semantic similarity. Finally, the 'best' three lines
are picked for the final Haiku, each one semantically involving one of the
three random keywords originally picked.

Another approach similar to my technique is the case based reasoning technique
developed in \cite{Gervas01}. This approach accepts prose sentences to be
converted to poetry. Largely, the process is Generative, making grammatical
changes to the input sentence as well as word substitutions. The target output
is specified by some existing stanza of the desired form (ie, a haiku). An
important part of the process, however, is a vocabulary of words that may be
substituted into the original prose to achieve the desired poetic structure,
making this approach a combination of Templatized and Generative techniques. A
user must supply this vocabulary for each poem desired.

\subsection{Analysis}
Throughout the course of this research it became evident that a common problem in
computer poetry was simply the question: Why? Templatized techniques tended to
rely almost exclusively on the human programmers designing the poetry
generation software; it seemed like the computer was little more than a set of
dice. In \cite{Manurung03}, where the Templatized technique is taken to an
extreme, the semantic scope of the produced work is still strictly limited to
what humans provide in the LTAG collection. It seems like computer poetry like
this isn't contributing anything to the field that a human couldn't already do
by hand.

The CBR approach suffers from similar problems despite being a partially Generative technique.
A large part of the creativity of the system depends on the words chosen for
the vocabulary used, meaning that a human must premeditate the content of
produced poems. Similarly, a human user must select the poetic verses used to
compare potential outputs to when generating a poem.

The question is easier to answer for Generative techniques. The Travesty
Engine, for example, produces new arrangements of pieces that humans may never
think to produce. It can reveal new meaning in an existing piece through purely
aleatoric means. Similarly, in the VSM Haiku generator, the produced poetry can
describe the zeitgeist of the blogs it searches (something that even Google
struggles to do). Clearly this poetry was doing something that a human couldn't
do: revealing meaning hidden within existing human prose. However, I submit
that in the former example the rearrangement is too simple; it doesn't go far
enough and its results become predictable after a fashion. In the latter
example, the final Haiku bear too little resemblance to their source material
to really articulate any kind of underlying meaning to the reader.

\section{Directed Cut-up Technique}
Thus, I present a new Generative technique inspired by the work of the poet and
author William Burroughs. This technique exploits the diversity and large
number of sentences found in large corpora--for example, Wikipedia, the online
articles of a newspaper, or online public domain books--by scanning the
sentences within them, profiling them, and then using them to populate a poem
based on rules input by a human user. Thus, the technique \emph{cuts up}, like
Burroughs \cite{wikiCutup}, some given corpus in order to produce a poetic
result. I argue that this technique produces interesting work, and present
several features of the resulting pieces that support this.

\subsection{Explication of Algorithm}
The technique is driven by a short algorithm  supported by several modules. It
is presented in pseudo-code in figure 1. The algorithm, given some corpus $C$
containing several thousands of lines of text, stores each sentence of the
corpus in a databse with associated syllabic and phonetic information. Using
any number of simple guidelines supplied by a user, it proceeds to construct a
poem using the sentences stored in the database. Should the algorithm be unable
to locate a sentence to satisfy some line of the desired poem--say, a sentence with
10 syllables containing the word \emph{aleatoric} and rhyming with the word
\emph{trumpet}--it \emph{weakens} the rule for that line, relaxing the criteria
used in the database lookup to find a sentence that at least closely resembles
one needed to meet the user's guidelines. Without this technique, termination
is not guaranteed; however, every rule eventually weakens to the trivial rule,
which is satisfied by any sentence.

\pagebreak
\onehalfspacing
\begin{figure}[here]
\begin{algorithmic}
\STATE Given some corpus $C$ and database handle $DB$
\STATE $L\gets$ normalize($C$)
\STATE insert($DB$, profile($L$))
\STATE $i \gets$ input()
\STATE $R \gets$ rulesParse($i$)
\STATE $P \gets \emptyset$
\FOR{$r \in R$}
    \STATE $S \gets$ select($DB$, ruleToQuery($r$))
    \IF {not $S$}
        \WHILE {$r\prime \gets$ weaken($r\prime$)}
            \STATE $S\prime \gets$ select($DB$, ruleToQuery($r\prime$))
            \IF { $S\prime$ }
                \STATE $S \gets S\prime$
                \STATE break
            \ENDIF
        \ENDWHILE
    \ENDIF
    \STATE $s \gets$ random($S$)
    \STATE push($P, s$)
\ENDFOR
\STATE output($P$)
\end{algorithmic}
\caption{My algorithm}
\label{fig:algorithm}
\end{figure}
\doublespacing

\subsection{Technical Implementation}
The tools involved in the implementation of this algorithm are Perl 5.10 and
SQLite3. Perl was chosen due to its rich Lingua::EN repository which contained
many modules for parsing, splicing, and cleaning up English text. SQLite3 was
chosen for its speed, in-RAM database feature and portability.

Despite initial concerns about the ability of an interpreted language to
process enough text in a short time, the implementation ended up with favorable
benchmarks. Producing a 10 line poem from a SQLite3 database containing .4
million sentences consistently took between .2 and .4 seconds. Parsing a 1
million line corpus and inserting it into a SQLite3 database took 2.5 hours.
The former benchmark indicates that the poem generation code will scale for
larger corpora, but the latter shows that refactoring or rewriting will be
necessary to support multimillion line corpora.

\subsection{Corpus Acquisition}
A key component of this approach is the presence of a large corpus. Years ago,
before the Internet expanded to its current volumnuous state, it is unlikely
that any such corpus would be publicly and freely available. Now, one can
consider the publicly accessible areas of the Internet as one huge corpus with
smaller selections of Internet content as subsets of this corpus.

For initial testing and debugging, a digitized version of a dozen short stories
by Jorge Luis Borges was manually developed and employed.

For later testing and protyping, a selection of 3,700 books from Project
Gutenberg were scraped using an automated script. From these books, 1,000,000
lines of text (out of a total 7,000,000) were analyzed, producing 433,850
profiled sentences stored in a database for use.

A scrape of several thousand Wikipedia pages was attempted, but the content
proved to be too diverse (in terms of encoding and structure) to be parsed
without lots of additional overhead.

\subsection{Corpus Processing}

\subsection{Rules System}
\subsubsection{Input to Rules Translation}
\subsubsection{Rules to Query Translation}
\subsection{Final Composition}
\section{Results}
\subsection{Selected outputs}
\subsection{Defense of Technique}

\subsubsection{Semantic Consistency}
By using one or more topics as rules in the input to the software one can
topically flavor the output without restricting it. Sentences relevant to the
selected topic will be used to produce the final piece. The reader should be
able to get a sense of what each produced poem is "about." It is important to
note that this semantic consistency does not undermine juxtaposition, one of
the important aspects of computer poetry \cite{Hart96}. The output will still
contain the often jarring combination of seemingly otherwise unrelated phrases
despite their semantic consistency.

\subsubsection{Variable Structure}
Poems produced by my technique can scale from completely arbitrary, random
productions to very narrowly defined poetic structures (like sonnets, Haiku, or
limericks). The rules engine system allows for great flexibility and is a key
component of this technique. Its success, however, relies on the theory that
large enough corpora will contain sentences diverse enough to satisfy any input
rules.

\subsubsection{Human Mediated, Not Human Authored}
A key aspect of my approach is that, while all of the content used to produce
works is written by humans, the meaning of the content is subverted and altered
by the reappropriation process to create something new in which the humans and
computers involved both played integral roles in the poetic production. I refer
to the technique as being \emph{human mediated} as humans provide the
media--prose sentences--while the computer acts as the artist. In this case,
however, the computer is more like an artist of reappropriation like a DJ,
collage artist, or cut-up author.

\section{Further Research}

\bibliography{paper}
\bibliographystyle{apa}

\end{document}
